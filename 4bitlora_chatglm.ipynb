{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ before you do it, make sure you have the latest version of transformer,bitsandbytes,peft,accelerate properly installed \n",
    "\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig,Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType,prepare_model_for_int8_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [01:43<00:00, 12.99s/it]\n"
     ]
    }
   ],
   "source": [
    "###### load model with bnb_4bit \n",
    "model = AutoModel.from_pretrained(\n",
    "        \"chatglm-6b\",\n",
    "        load_in_4bit=True,\n",
    "        device_map='auto',\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        ),\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "###### froze model weights before adding lora layers\n",
    "model = prepare_model_for_int8_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gupo/anaconda3/envs/rabbit_lm/lib/python3.8/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### identify which layers from the model are good to add lora layers for\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit \n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "########### create lora layers and add them to relevant layers in the model\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### further adjust dtype for some layers in the model \n",
    "def update_somelayers_dtype(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer): \n",
    "            module = module.to(torch.bfloat16)\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.bfloat16)  ###### in the qlora library this is set to float32. Setting to bf16 may cause problems\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n",
    "\n",
    "update_somelayers_dtype(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### ok load my data. There are two columns: input, output, for example:\n",
    "#####################  input                    output\n",
    "#####################  hello                   how are you?\n",
    "#####################  what's your name ?      Eion Mask\n",
    "\n",
    "dataset = pd.read_csv('rabbit.csv')[['input','output']]\n",
    "dataset = dataset.rename({'input':'context','output':'target'},axis=1)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "\n",
    "###################### tokenize the input and output\n",
    "def preprocess(example):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=1024, truncation=True)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target, max_length=512, truncation=True, add_special_tokens=False\n",
    "    )\n",
    "    input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    example['input_ids'] = input_ids\n",
    "    example['seq_len'] = len(prompt_ids)\n",
    "    return example\n",
    "   \n",
    "\n",
    "tokenized_dataset = dataset.map(function=preprocess,remove_columns=['context','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### this is the messy data collator\n",
    "##################### to understand how it works, \n",
    "####################  you need to check the GLM paper about \n",
    "################### how attention masks,  position embeddings,etc., are organized\n",
    "\n",
    "def get_masks_and_position_ids(\n",
    "    seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
    "):\n",
    "    mask_position = (\n",
    "        seq_len - 2\n",
    "    )  \n",
    "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., : mask_position - 1] = 1\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[seq_length:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (\n",
    "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
    "                torch.arange(\n",
    "                    context_length - seq_length, dtype=torch.long, device=device\n",
    "                )\n",
    "                + 1,\n",
    "            )\n",
    "        )\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[context_length - 1 :] = mask_position\n",
    "    return attention_mask, position_ids\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    # print(features)\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids) + 1\n",
    "    input_ids = []\n",
    "    attention_mask_list = []\n",
    "    position_ids_list = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1)\n",
    "            + ids[(seq_len - 1) :]\n",
    "            + [tokenizer.eos_token_id]\n",
    "            + [-100] * (longest - ids_l - 1)\n",
    "        )\n",
    "        ids = ids + [tokenizer.eos_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        attention_mask, position_ids = get_masks_and_position_ids(\n",
    "            ids, seq_len, longest, _ids.device, gmask=False\n",
    "        )\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        position_ids_list.append(position_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(attention_mask_list)\n",
    "    position_ids = torch.stack(position_ids_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# I don't care evalution. So I set it eval_data to none. Pass your actual eval_data here. \n",
    "################# Do remember to set remove_unused_columns to false \n",
    "################# otherwise it keeps complaining that the seq_len feature is missing! \n",
    "\n",
    "outdir ='chatglm-rabbit'\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=32,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=100,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=250,\n",
    "        output_dir=outdir,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy= \"no\",\n",
    "        eval_steps= None,\n",
    "        report_to='none',\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='2919' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 102/2919 44:27 < 20:52:19, 0.04 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.956700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rabbit_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
